{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP language model using embeddings\n",
    "\n",
    "Based on 1000 boy names, see how a language model manages to generate new random ones based on a MLP network using embeddings for each character. Code inspired by https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb and network based on https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 names loaded. Examples: ['Aarav', 'Aaron', 'Abdiel']..\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "    print(f\"{len(names)} names loaded. Examples: {names[:3]}..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab\n",
    "\n",
    "- Store all unique characters in a vocab. \n",
    "    - Also add a unique token indicating that a word starts and ends.\n",
    "- Keep capital letters (So we can see if model is smart enough to start names with capital later)\n",
    "- Create dicts to help us map between chars and index in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size 54\n",
      "Index of character 'x' = 50\n",
      "Character of index 26 = Z\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = \"(\", \")\"\n",
    "vocab = [START_TOKEN] + sorted(list(set(\"\".join(names)))) + [END_TOKEN]\n",
    "print(f\"Vocab of size {len(vocab)}\")\n",
    "# Create \"string to index\" and \"index to string\" dicts for lookup purposes\n",
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "print(f\"Index of character 'x' = {stoi['x']}\")\n",
    "print(f\"Character of index 26 = {itos[26]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Given a torch tensor containing `context_size` number of previous characters, the network should output the probabilities of each character in the vocab to come after the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(names, context_size=3):\n",
    "    # context_size: Number of characters as context when predicting next word\n",
    "    X, Y = [], [] # Given X, predict Y\n",
    "    for name in names:\n",
    "        # First character in a name have padded START_TOKEN as context\n",
    "        context = [stoi[START_TOKEN]] * context_size \n",
    "        for char in name + END_TOKEN: # E.g. A, d, a, m, )\n",
    "            idx = stoi[char] \n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            # Context for next char in the name\n",
    "            context = context[1:] + [idx]\n",
    "    X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([6666, 3]), Y: torch.Size([6666])\n"
     ]
    }
   ],
   "source": [
    "X, Y = init_dataset(names, context_size=3)\n",
    "print(f\"X: {X.shape}, Y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given tensor([31, 35, 38]) predict 53\n",
      "['e', 'i', 'l'] -> )\n",
      "Given tensor([0, 0, 0]) predict 10\n",
      "['(', '(', '('] -> J\n",
      "Given tensor([ 0,  2, 51]) predict 44\n",
      "['(', 'B', 'y'] -> r\n",
      "Given tensor([0, 0, 0]) predict 26\n",
      "['(', '(', '('] -> Z\n",
      "Given tensor([27, 39, 27]) predict 44\n",
      "['a', 'm', 'a'] -> r\n"
     ]
    }
   ],
   "source": [
    "for i in torch.randint(0, X.shape[0], (5,)): # 5 random samples in X\n",
    "    print(f\"Given {X[i]} predict {Y[i]}\")\n",
    "    print(f\"{[itos[j] for j in X[i].tolist()]} -> {itos[Y[i].item()]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Based on https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "\n",
    "<img src=\"../assets/network.png\"  alt=\"Neural Network Architecture\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10\n",
    "context_size = 3\n",
    "layer1_size = 100\n",
    "\n",
    "C = torch.randn((vocab_size, embedding_dim)) # Embeddings\n",
    "W1 = torch.randn((context_size * embedding_dim, layer1_size))\n",
    "b1 = torch.randn(layer1_size)\n",
    "W2 = torch.randn((W1.shape[1], vocab_size))\n",
    "b2 = torch.randn(vocab_size)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding table torch.Size([54, 10]) meaning each of the 54 tokens in the vocab have embedding vector of shape 10\n",
      "Example: embeddings of vocab item i=6 (F) = tensor([ 1.7801, -1.0107,  1.0148,  0.2952,  0.2075,  0.1045,  0.5826,  1.2838,\n",
      "        -0.6250, -0.5551])\n",
      "\n",
      "Weights in layer 1 with shape torch.Size([30, 100]) + bias of shape torch.Size([100]) meaning 30 numbers go into this layer and 100 goes out. The 30 inputs from embeddings table is because the network is configured to take 3 char as context and each char have 10 embeddings, I.e. 3*10=30\n",
      "\n",
      "Weights in output layer is of shape torch.Size([100, 54]) + bias of shape torch.Size([54]) meaning the 100 outputs from layer 1 goes in and it outputs 54 numbers indicating the probability of each token in the vocab of size 54\n",
      "\n",
      "Total number of parameters: 9094\n"
     ]
    }
   ],
   "source": [
    "print((\n",
    "    f\"Embedding table {C.shape} meaning each of the {C.shape[0]} tokens in the vocab \"\n",
    "    f\"have embedding vector of shape {C.shape[1]}\\n\"\n",
    "    f\"Example: embeddings of vocab item i=6 ({itos[6]}) = {C[6]}\\n\"\n",
    "))\n",
    "print(\n",
    "    f\"Weights in layer 1 with shape {W1.shape} + bias of shape {b1.shape} \"\n",
    "    f\"meaning {W1.shape[0]} numbers go into this layer and {W1.shape[1]} goes out. \"\n",
    "    f\"The {W1.shape[0]} inputs from embeddings table is because the network is configured to \"\n",
    "    f\"take {context_size} char as context and each char have {embedding_dim} embeddings, I.e. \"\n",
    "    f\"{context_size}*{embedding_dim}={W1.shape[0]}\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Weights in output layer is of shape {W2.shape} + bias of shape {b2.shape} \"\n",
    "    f\"meaning the {W2.shape[0]} outputs from layer 1 goes in and it outputs \"\n",
    "    f\"{W2.shape[1]} numbers indicating the probability of each token in the vocab of size {vocab_size}\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Total number of parameters: {sum(p.nelement() for p in parameters)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Setup train and validation datasets to identify over/under fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([5345, 3]), X_val: torch.Size([1321, 3])\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(names)\n",
    "train_size = .8 # Train on 80% of the names\n",
    "cutoff = int(0.8*len(names))\n",
    "X_train, y_train = init_dataset(names[:cutoff], context_size=3)\n",
    "X_val, y_val = init_dataset(names[cutoff:], context_size=3)\n",
    "print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch of shape torch.Size([8]):\n",
      "tensor([  47, 1258, 1472, 1753, 3547, 4945, 4154, 3357])\n",
      "\n",
      "The X_train indexed by minibatch have shape torch.Size([8, 3])\n",
      "Using the X_train minibatch to index embeddings gives shape torch.Size([8, 3, 10]) (This needs to be reshaped to torch.Size([8, 30]) (batch_size, num_in_layer_1))\n",
      "\n",
      "The output of layer 1, h=torch.Size([8, 100]) (batch_size, num_out_layer_1)\n",
      "\n",
      "We get logits of shape torch.Size([8, 54]) (batch_size, vocab_size) after passing h through layer 2\n",
      "\n",
      "We compute loss=16.273134231567383 with logits and y_train minibatch\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "batch_idxs = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "embeddings = C[X_train[batch_idxs]]\n",
    "h = torch.tanh(embeddings.view(-1, W1.shape[0]) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, y_train[batch_idxs])\n",
    "\n",
    "print(f\"Minibatch of shape {batch_idxs.shape}:\\n{batch_idxs}\\n\")\n",
    "print(f\"The X_train indexed by minibatch have shape {X_train[batch_idxs].shape}\")\n",
    "print(\n",
    "    f\"Using the X_train minibatch to index embeddings gives shape {embeddings.shape}\"\n",
    "    f\" (This needs to be reshaped to {embeddings.view(-1, 30).shape} (batch_size, num_in_layer_1))\\n\"\n",
    ")\n",
    "print(f\"The output of layer 1, h={h.shape} (batch_size, num_out_layer_1)\\n\")\n",
    "print(f\"We get logits of shape {logits.shape} (batch_size, vocab_size) after passing h through layer 2\\n\")\n",
    "print(f\"We compute loss={loss} with logits and y_train minibatch\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is the loss derived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets use the first sample from the minibatch as example (index 47)\n",
      "The logits outputs probabilities for each char in the vocab:\n",
      "tensor([  5.6044,   2.5534,   3.9820,  -4.5186,  -0.2839,  -1.0272,  -1.7683,\n",
      "          1.4357,  -3.9431,  -3.6824,  -7.7552,  25.0023,  -9.9718, -13.8469,\n",
      "         -1.4932,  -5.3721, -13.6540, -22.9122,  13.9443,  -2.9753,   2.5469,\n",
      "         -4.8560,   9.2427,   8.0669,   3.6398,   7.2906,   6.4026,   4.6832,\n",
      "         -5.7410,   7.3943, -15.3667, -12.9856,  -6.1083,  -4.7998,  -2.6998,\n",
      "          8.2127,  17.6186,  -8.3527,  19.1753,   4.7208,  -1.5858,  16.2543,\n",
      "         -7.6275, -20.3351,  -0.7546,  -2.0605,  10.8597,   6.5540,  -5.3622,\n",
      "          3.6679,  21.2206,   6.5635,   5.4483,  -7.5663])\n",
      "\n",
      "The true char index is 36, which is 17.618572235107422 in the logits\n",
      "We want the logits to be probabilities between 0 and 1 and they should sum up to 1\n",
      "The 36th logit should then be as close to 1 as possible to minimize loss (most probable char)\n",
      "\n",
      "We start by taking the exponential of the logits, which gives us these counts:\n",
      "tensor([2.7161e+02, 1.2850e+01, 5.3623e+01, 1.0904e-02, 7.5284e-01, 3.5800e-01,\n",
      "        1.7062e-01, 4.2027e+00, 1.9388e-02, 2.5164e-02, 4.2853e-04, 7.2168e+10,\n",
      "        4.6701e-05, 9.6911e-07, 2.2465e-01, 4.6445e-03, 1.1753e-06, 1.1204e-10,\n",
      "        1.1374e+06, 5.1034e-02, 1.2768e+01, 7.7819e-03, 1.0329e+04, 3.1872e+03,\n",
      "        3.8086e+01, 1.4664e+03, 6.0344e+02, 1.0812e+02, 3.2114e-03, 1.6268e+03,\n",
      "        2.1200e-07, 2.2932e-06, 2.2244e-03, 8.2314e-03, 6.7218e-02, 3.6875e+03,\n",
      "        4.4838e+07, 2.3575e-04, 2.1268e+08, 1.1225e+02, 2.0479e-01, 1.1459e+07,\n",
      "        4.8689e-04, 1.4743e-09, 4.7019e-01, 1.2739e-01, 5.2034e+04, 7.0207e+02,\n",
      "        4.6905e-03, 3.9169e+01, 1.6443e+09, 7.0878e+02, 2.3237e+02, 5.1759e-04])\n",
      "\n",
      "Next step is to convert these counts to probabilites between 0 and 1.\n",
      "We calculate a probability matrix by dividing each item by the sum of the counts\n",
      "The sum of the counts is 74082312192.0, giving us probabilities:\n",
      "tensor([3.6663e-09, 1.7346e-10, 7.2383e-10, 1.4719e-13, 1.0162e-11, 4.8325e-12,\n",
      "        2.3031e-12, 5.6730e-11, 2.6171e-13, 3.3967e-13, 5.7844e-15, 9.7416e-01,\n",
      "        6.3039e-16, 1.3082e-17, 3.0324e-12, 6.2694e-14, 1.5865e-17, 1.5124e-21,\n",
      "        1.5353e-05, 6.8888e-13, 1.7235e-10, 1.0504e-13, 1.3943e-07, 4.3023e-08,\n",
      "        5.1410e-10, 1.9794e-08, 8.1455e-09, 1.4594e-09, 4.3349e-14, 2.1959e-08,\n",
      "        2.8617e-18, 3.0954e-17, 3.0025e-14, 1.1111e-13, 9.0734e-13, 4.9776e-08,\n",
      "        6.0525e-04, 3.1823e-15, 2.8709e-03, 1.5153e-09, 2.7643e-12, 1.5468e-04,\n",
      "        6.5723e-15, 1.9901e-20, 6.3468e-12, 1.7196e-12, 7.0239e-07, 9.4769e-09,\n",
      "        6.3315e-14, 5.2873e-10, 2.2196e-02, 9.5674e-09, 3.1366e-09, 6.9867e-15])\n",
      "\n",
      "The probability of the true char was in the logits is then 0.0006052491371519864 (want to be close to 1)\n",
      "We compute the loss by taking the minus of the log of the probability -log(0.0006052491371519864)=7.409870624542236\n",
      "If prob=1, log(1)=0.0, if prob=0.001, log(0.001)=-6.907755374908447\n",
      "\n",
      "We can get the same loss by using the torch cross_entropy function: 7.409870624542236\n"
     ]
    }
   ],
   "source": [
    "ex = batch_idxs[0]\n",
    "ex_true = y_train[ex]\n",
    "print(f\"Lets use the first sample from the minibatch as example (index {ex})\")\n",
    "print(f\"The logits outputs probabilities for each char in the vocab:\\n{logits[0]}\\n\")\n",
    "print(f\"The true char index is {ex_true}, which is {logits[0][ex_true]} in the logits\")\n",
    "print(\"We want the logits to be probabilities between 0 and 1 and they should sum up to 1\")\n",
    "print(f\"The {ex_true}th logit should then be as close to 1 as possible to minimize loss (most probable char)\")\n",
    "counts = logits[0].exp()\n",
    "print(f\"\\nWe start by taking the exponential of the logits, which gives us these counts:\\n{counts}\\n\")\n",
    "prob = counts / counts.sum() # counts / counts.sum(1, keepdims=True) when matrix\n",
    "print(\"Next step is to convert these counts to probabilites between 0 and 1.\")\n",
    "print(\"We calculate a probability matrix by dividing each item by the sum of the counts\")\n",
    "print(f\"The sum of the counts is {counts.sum()}, giving us probabilities:\\n{prob}\\n\")\n",
    "print(f\"The probability of the true char was in the logits is then {prob[ex_true]} (want to be close to 1)\")\n",
    "print(\n",
    "    f\"We compute the loss by taking the minus of the log of the probability \"\n",
    "    f\"-log({prob[ex_true]})={-prob[ex_true].log()}\"\n",
    ")\n",
    "print(f\"If prob=1, log(1)={torch.tensor(1).log()}, if prob=0.001, log(0.001)={torch.tensor(0.001).log()}\\n\")\n",
    "loss = F.cross_entropy(logits[0], y_train[batch_idxs][0])\n",
    "print(f\"We can get the same loss by using the torch cross_entropy function: {loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12e93fbb0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_iter = 100000\n",
    "train_loss = []\n",
    "\n",
    "# We want to calculate gradients for all parameters when training\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "for i in range(num_iter):\n",
    "    batch_idxs = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "\n",
    "    embeddings = C[X_train[batch_idxs]]\n",
    "    h = torch.tanh(embeddings.view(-1, W1.shape[0]) @ W1 + b1) \n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, y_train[batch_idxs])\n",
    "\n",
    "    # Backward\n",
    "    for p in parameters: # Reset gradients before backward pass\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update gradients with sharper learning rate in the start\n",
    "    lr = 0.01 if i < 1000 else 0.05\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "plt.plot([i for i in range(num_iter)], train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3442747592926025,\n",
       " 2.162874460220337,\n",
       " 1.150926947593689,\n",
       " 0.40205705165863037,\n",
       " 1.4311473369598389,\n",
       " 1.3240506649017334,\n",
       " 2.4455506801605225,\n",
       " 1.1595568656921387,\n",
       " 2.565973997116089,\n",
       " 1.6574114561080933]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss[-10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did it overfit?\n",
    "\n",
    "Training loss and validation loss should be similar. If train loss is way below valid, it is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6387, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X_train]\n",
    "h = torch.tanh(embeddings.view(-1, W1.shape[0]) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2754, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X_val]\n",
    "h = torch.tanh(embeddings.view(-1, W1.shape[0]) @ W1 + b1) \n",
    "logits = h @ W2 + b2 \n",
    "loss = F.cross_entropy(logits, y_val)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate names\n",
    "\n",
    "- Should be able to start each name with capital letter\n",
    "- The loop keeps generating char by char until the END_TOKEN is sampled. \n",
    "    - Each char is sampled from the probability distribution that the network outputs, given the context tensor as input.\n",
    "- High probability that many of the generated names exist in the dataset it is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Koce (Generated)\n",
      "Mr. Theo (In dataset)\n",
      "Mr. Tobert (Generated)\n",
      "Mr. Matliel (Generated)\n",
      "Mr. Mohammaded (Generated)\n",
      "Mr. Moison (Generated)\n",
      "Mr. Noel (In dataset)\n",
      "Mr. Matum (Generated)\n",
      "Mr. Kane (In dataset)\n",
      "Mr. Hasriel (Generated)\n",
      "Mr. Harry (In dataset)\n",
      "Mr. Kairo (In dataset)\n",
      "Mr. Nicos (Generated)\n",
      "Mr. Kall (Generated)\n",
      "Mr. Adeustus (Generated)\n",
      "Mr. Quint (Generated)\n",
      "Mr. Frantii (Generated)\n",
      "Mr. Matticus (Generated)\n",
      "Mr. Ksrius (Generated)\n",
      "Mr. Kudlan (Generated)\n",
      "Mr. Mackson (Generated)\n",
      "Mr. Garryne (Generated)\n",
      "Mr. Karson (In dataset)\n",
      "Mr. Chris (In dataset)\n",
      "Mr. Maxiogo (Generated)\n",
      "Mr. Feres (Generated)\n",
      "Mr. Mohammanuel (Generated)\n",
      "Mr. Ari (In dataset)\n",
      "Mr. Karence (Generated)\n",
      "Mr. Kaleso (Generated)\n"
     ]
    }
   ],
   "source": [
    "num = 30\n",
    "for _ in range(num):\n",
    "    name = []\n",
    "    context = [stoi[START_TOKEN]] * context_size \n",
    "    while True:\n",
    "        embeddings = C[torch.tensor([context])] # (1, context_size, embedding_dim)\n",
    "        h = torch.tanh(embeddings.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [idx]\n",
    "        if idx == stoi[END_TOKEN]: break\n",
    "        name.append(itos[idx])\n",
    "    name = ''.join(name)\n",
    "    exist = \"(In dataset)\" if name in names else \"(Generated)\"\n",
    "    print(f\"Mr. {name} {exist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_bigram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdb541eee026b553c5777d251e843d26bc60d4a753b37888e8eeecefbd9d6ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
